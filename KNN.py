# -*- coding: utf-8 -*-
"""23110058_PA2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XTO2tNliyZIr5hOEqihdL3QwwVI1iNk8

# CS535/EE514 Machine Learning - Spring 2022 - PA2

## Instructions 

*   Submit your code both as notebook file (.ipynb) and python script (.py) on LMS. The name of both files should be 'RollNo_PA2'. Failing to submit any one of them will result in the reduction of marks.
*  All the cells must be run once before submission and should be displaying the results(graphs/plots etc). If output of the cells is not being displayed, marks will be dedcuted.
*   The code MUST be implemented independently. Any plagiarism or cheating of work from others or the internet will be immediately referred to the DC.
* 10% penalty per day for 3 days after due date. No submissions will be accepted
after that.  
* Use procedural programming style and comment your code properly.
* **Deadline to submit this assignment is 21/02/2022.**

## Part 1: Implement K-NN classifier from scratch 
The goal of this assignment is to get you familiar with k-NN classification and to give hands on experience of basic python tools and libraries which will be used in implementing the algorithm.
You are not allowed to use scikit-learn or any other machine learning toolkit for this part.
You have to implement your own k-NN classifier from scratch. You may use Pandas, NumPy, Matplotlib and other standard python libraries.
"""

# making all necessary imports here. You can make your own imports here if needed.
import glob
import math 
import cv2
import os
from google.colab import drive
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import image as mpimg
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
from sklearn.datasets import make_blobs
from sklearn.model_selection import GridSearchCV
from google.colab import drive
drive.mount('/content/drive/')

# Downloading Dataset
# !gdown --id 1ZPLfYxdXs856KsjcBgG2QGjjr6ayPwVj
# !unzip "Apparel_Dataset.zip"
#the data set was downloaded and uploaded to drive manually as it could not be accessed through this cell (access denied error)

"""You have been given Apparel Dataset which contains images of dress, pants, shirts and shoes. There are two top-level directories "Test Data" and "Train Data" corresponding to test set and training set respectively.  Each of these directories further contains four directories "dress", "pants", "shirts" and "shoes". The class labels of each of the
images correspond to the directory they are contained in i.e., dress/pants/shirts/shoes. Paths of training and test data have been given to you. 
"""

test_data_path = "drive/MyDrive/Apparel_Dataset/Test Data"
train_data_path = "drive/MyDrive/Apparel_Dataset/Train Data"

"""### TASK 1:  
Now you need to read the images and resize all of them to a fixed size (32,32). After resizing the images, flatten them to create a 1-D array and then normalize the pixel values between 0-1. This 1-D array will serve as your feature vector for the particular image.

Note: You can normalize the pixel values by dividing them by 255.





"""

### YOUR CODE HERE ###

#FOR Train DATA

dress_path = train_data_path + "/dress"
dress_folder = os.walk(dress_path)
pants_path = train_data_path + "/pants"
pants_folder = os.walk(pants_path)                  #setting paths for different folders
shirts_path = train_data_path + "/shirts"
shirts_folder = os.walk(shirts_path)
shoes_path = train_data_path + "/shoes"
shoes_folder = os.walk(shoes_path)


dresses = set()
dress_list = []
pants_list = []
shirts_list = []
shoes_list = []       #empty lists to be appended with appropriate image data
training_data_dictionary = {
    "dress": None,
    "pants": None,        #a dictionary used to store the data, dictionary was used as it is easier to access data from it
    "shirts": None,
    "shoes": None,
}


for (root,dirs,files) in dress_folder:
  for name in files:
    img = np.array(mpimg.imread(dress_path+'/'+name))
    # image = cv2.imread(dress_path+'/'+name)
    img = cv2.resize(img,(32,32))
    img = img.flatten()
    img = np.true_divide(img,255)     #this for loop and the following loop through all the images and append the data in the respective lists
    dress_list.append(img)

training_data_dictionary["dress"] = dress_list      #training data dictionary updated with all the image data for dresses


for (root,dirs,files) in pants_folder:        #similarly as above, the rest of the folders appended to lists and added to the dictioanry
  for name in files:
    img = np.array(mpimg.imread(pants_path+'/'+name))
    img = cv2.resize(img,(32,32))
    img = img.flatten()
    img = np.true_divide(img,255)
    pants_list.append(img)

training_data_dictionary["pants"] = pants_list

for (root,dirs,files) in shirts_folder:
  for name in files:
    img = np.array(mpimg.imread(shirts_path+'/'+name))
    img = cv2.resize(img,(32,32))
    img = img.flatten()
    img = np.true_divide(img,255)
    shirts_list.append(img)

training_data_dictionary["shirts"] = shirts_list

for (root,dirs,files) in shoes_folder:
  for name in files:
    img = np.array(mpimg.imread(shoes_path+'/'+name))
    img = cv2.resize(img,(32,32))
    img = img.flatten()
    img = np.true_divide(img,255)
    shoes_list.append(img)

training_data_dictionary["shoes"] = shoes_list


# For Test Data similar methods used

dress_path = test_data_path + "/dress"
dress_folder = os.walk(dress_path)
pants_path = test_data_path + "/pants"
pants_folder = os.walk(pants_path)
shirts_path = test_data_path + "/shirts"
shirts_folder = os.walk(shirts_path)
shoes_path = test_data_path + "/shoes"
shoes_folder = os.walk(shoes_path)

dress_list = []
pants_list = []
shirts_list = []
shoes_list = []
test_data_dictionary = {
    "dress": None,
    "pants": None,
    "shirts": None,
    "shoes": None,
}


for (root,dirs,files) in dress_folder:
  for name in files:
    img = np.array(mpimg.imread(dress_path+'/'+name))
    # image = cv2.imread(dress_path+'/'+name)
    img = cv2.resize(img,(32,32))
    img = img.flatten()
    img = np.true_divide(img,255)
    dress_list.append(img)

test_data_dictionary["dress"] = dress_list

for (root,dirs,files) in pants_folder:
  for name in files:
    img = np.array(mpimg.imread(pants_path+'/'+name))
    img = cv2.resize(img,(32,32))
    img = img.flatten()
    img = np.true_divide(img,255)
    pants_list.append(img)

test_data_dictionary["pants"] = pants_list

for (root,dirs,files) in shirts_folder:
  for name in files:
    img = np.array(mpimg.imread(shirts_path+'/'+name))
    img = cv2.resize(img,(32,32))
    img = img.flatten()
    img = np.true_divide(img,255)
    shirts_list.append(img)

test_data_dictionary["shirts"] = shirts_list

for (root,dirs,files) in shoes_folder:
  for name in files:
    img = np.array(mpimg.imread(shoes_path+'/'+name))
    img = cv2.resize(img,(32,32))
    img = img.flatten()
    img = np.true_divide(img,255)
    shoes_list.append(img)

test_data_dictionary["shoes"] = shoes_list



len(training_data_dictionary["dress"][0]) #32x32x3 3072

"""### TASK 2: 
Create your own k-Nearest Neighbors classifier function by performing following
tasks: 

*   For a test data point, find its distance from all training instances.
*   Sort the calculated distances in ascending order based on distance values.
*   Choose k training samples with minimum distances from the test data point.
*   Return the most frequent class of these samples. (Incase of ties, break them by backing off to k-1 values. For example, for a particular image, incase of k=4, you get two 'dress' labels and two 'pants' labels. In this case you will break tie by backing off to k=3. If tie occurs again you will keep backing off until tie is broken or you reach k=1.)
*   Note: Your function should work with Euclidean distance as well as Manhattan
distance. Pass the distance metric as a parameter in k-NN classifier function.
Your function should also be general enough to work with any value of k.
"""

### YOUR CODE HERE ###

def manhattan_distance(arr1,arr2):
  manhattan_array = np.subtract(arr1,arr2)
  return np.sum(np.abs(manhattan_array))      #manhattan distance, subtracting two arrays and taking the sum of the absolute values of an array

def euclidean_distance(arr1,arr2):
  temp_array = np.subtract(arr1,arr2) 
  temp_array = np.square(temp_array)    #similarly for euclidean distance, subtracting two arrays, squaring each element from the new array and taking the sqrt of their sum
  return math.sqrt(np.sum(temp_array))

def get_largest(dress,shoes,shirts,pants):
  maximum = max(dress,shoes,shirts,pants)
  list_final = []
  if dress == maximum:
    list_final.append("dress")
  if shoes == maximum:
    list_final.append("shoes")
  if shirts == maximum:
    list_final.append("shirts")
  if pants == maximum:
    list_final.append("pants")
  if len(list_final) > 1:
    return "rerun"
  elif len(list_final) == 1:
    return list_final[0]
  
  


def KNNfunc(test_point,training_data_dictionary,distance_to_use,k):
  dress_points = []
  shoes_points = []                                                                                         
  shirts_points = []
  pants_points = []
  for arr in training_data_dictionary["dress"]:
    dress_points.append(("dress",(distance_to_use(test_point,arr))))    #calculating the distance from the test point to each data point in the training set of the dresses folder
  for arr in training_data_dictionary["pants"]:
    pants_points.append(("pants",(distance_to_use(test_point,arr))))
  for arr in training_data_dictionary["shoes"]:
    shoes_points.append(("shoes",(distance_to_use(test_point,arr))))
  for arr in training_data_dictionary["shirts"]:
    shirts_points.append(("shirts",distance_to_use(test_point,arr)))

  merged_data = dress_points + shoes_points + shirts_points + pants_points
  merged_data = tuple(merged_data)        #merging the data in a tuple of the format (class, distance)
  merged_data =sorted(merged_data,key = lambda x:x[1]) #sorting based on the distance values

  while k >= 1:
    closest_k = merged_data[0:k]
    dress_count = 0 
    shoes_count = 0
    shirts_count = 0
    pants_count = 0         # a loop which keeps running while k>=1
    for x in closest_k:
      if x[0] == "dress":
        dress_count = dress_count+1
      elif x[0] == "pants":
        pants_count = pants_count + 1
      elif x[0] == "shirts":
        shirts_count = shirts_count + 1
      elif x[0] == "shoes":
        shoes_count = shoes_count+1
    knn = get_largest(dress_count,shoes_count,shirts_count,pants_count) #this function returns the class which has the highest count, rerun is returned in case of 2 3 or 4 classes having the same highest count
    if knn == "rerun":
      k = k-1   #if two three or four classes have the highest count, we run the loop for (k-1)
    else:
       return knn

"""### TASK 3: 
Write down functions which calculates the classification accuracy, macro-average precision and macro-average recall. Print the values of accuracy, precision and recall calculated on the test data for k = 3,5,7 for both Euclidean and Manhattan distance. You should expect the values of accuracy, precision and recall to be above 80.
"""

### YOUR CODE HERE ###

#Helper Functions
def accuracy_helper(string,distance_to_use,k):
  list1 = []
  for x in test_data_dictionary[string]:
    list1.append(KNNfunc(x,training_data_dictionary,distance_to_use,k))
  return list1      #returns the predicted classes of a given array of test points

def number_of_times_correct(string,list_check):
  score = 0
  for i in list_check:
    if i == string:
      score = score+1
  return score #used to check how many times a predicted class has correct prediction



def false_positive_helper(string,list1,list2,list3):
  FalsePositive = 0
  for x in list1:
    if x == string:
      FalsePositive = FalsePositive+1
  for x in list2:
    if x == string:
      FalsePositive = FalsePositive + 1
  for x in list3:
    if x == string:           #helper to calculate false positive
      FalsePositive = FalsePositive + 1
  return FalsePositive

def true_negative_helper(string,list1,list2,list3):
  TrueNegative = 0
  for x in list1:
    if x != string:
      TrueNegative = TrueNegative+1
  for x in list2:
    if x != string:
      TrueNegative = TrueNegative + 1 #Helper to calculate true negative
  for x in list3:
    if x != string:
      TrueNegative = TrueNegative + 1
  return TrueNegative

def confusionMatrix(distance_to_use,k):
  list_of_matrix = []
  list1 = accuracy_helper("dress",distance_to_use,k)
  list2 = accuracy_helper("pants",distance_to_use,k)
  list3 = accuracy_helper("shoes",distance_to_use,k)
  list4 = accuracy_helper("shirts",distance_to_use,k) #get predicted classes for the test data
  #for dresses
  TruePositive = number_of_times_correct("dress",list1) #get true positive for dresses
  FalseNegative = len(list1) - TruePositive #false negative calculated by subtracting it from the total positive values

  TrueNegative = true_negative_helper("dress",list2,list3,list4) #true negative helper used, which counts the number of times dress does not occur in different lists
  FalsePositive = false_positive_helper("dress",list2,list3,list4) # counts the number of times dress occurs in the different lists 
  list_of_matrix.append(np.matrix([[TruePositive,FalsePositive],[FalseNegative, TrueNegative]])) #added to the matrix list 
  #similarly for other classes
  #for pants
  TruePositive = number_of_times_correct("pants",list2)
  FalseNegative = len(list2) - TruePositive

  TrueNegative = true_negative_helper("pants",list1,list3,list4)
  FalsePositive = false_positive_helper("pants",list1,list3,list4)
  list_of_matrix.append(np.matrix([[TruePositive, FalsePositive],[FalseNegative, TrueNegative]]))
  #for shoes 
  TruePositive = number_of_times_correct("shoes",list3)
  FalseNegative = len(list3) - TruePositive

  TrueNegative = true_negative_helper("shoes",list1,list2,list4)
  FalsePositive = false_positive_helper("shoes",list1,list2,list4)
  list_of_matrix.append(np.matrix([[TruePositive, FalsePositive],[FalseNegative, TrueNegative]]))
  #for shirts
  TruePositive = number_of_times_correct("shirts",list4)
  FalseNegative = len(list4) - TruePositive

  TrueNegative = true_negative_helper("shirts",list1,list2,list3)
  FalsePositive = false_positive_helper("shirts",list1,list2,list3)
  list_of_matrix.append(np.matrix([[TruePositive, FalsePositive],[FalseNegative, TrueNegative]]))
  
  return list_of_matrix

#CLASSIFICATION ACCURACY  

def classification_accuracy(distance_to_use,k):
  list1 = confusionMatrix(distance_to_use,k)
  dress = list1[0]
  pants = list1[1]
  shoes = list1[2]  #using the formula of classification accuracy, number of times you predicted correctly divided by total positives
  shirts = list1[3]
  num1 = dress[0,0] + pants[0,0]+shoes[0,0]+shirts[0,0]
  num2 = dress[1,0] + pants[1,0]+shoes[1,0]+shirts[1,0]
  return (num1/(num1+num2))

#Macro Average Precision

def macro_average_precision(distance_to_use,k):
  list1 = confusionMatrix(distance_to_use,k)
  dress = list1[0]
  pants = list1[1]
  shoes = list1[2]
  shirts = list1[3]

  num1 = dress[0,0]
  num2 = dress[0,1]
  dress_prec = (num1/(num1+num2))
                                          #calculating precision of each class using their matrixes, adding and dividing them by 4 to get average which is macro average precision
  num1 = pants[0,0]
  num2 = pants[0,1]
  pants_prec = (num1/(num1+num2))

  num1 = shoes[0,0]
  num2 = shoes[0,1]
  shoes_prec = (num1/(num1+num2))

  num1 = shirts[0,0]
  num2 = shirts[0,1]
  shirts_prec = (num1/(num1+num2))

  return ((dress_prec + pants_prec + shoes_prec + shirts_prec)/4)



##### Macro Average Recall

def macro_average_recall(distance_to_use,k):
  list1 = confusionMatrix(distance_to_use,k)
  dress = list1[0]
  pants = list1[1]
  shoes = list1[2]
  shirts = list1[3]

  num1 = dress[0,0]
  num2 = dress[1,0]
  dress_recall = (num1/(num1+num2))

  num1 = pants[0,0]
  num2 = pants[1,0]
  pants_recall = (num1/(num1+num2))

  num1 = shoes[0,0]
  num2 = shoes[1,0]                 #calculating recall of each class using their matrixes, adding and dividing them by 4 to get average which is the macro average recall
  shoes_recall = (num1/(num1+num2))

  num1 = shirts[0,0]
  num2 = shirts[1,0]
  shirts_recall = (num1/(num1+num2))

  return ((dress_recall + pants_recall + shoes_recall + shirts_recall)/4)



print("For Euclidean Distance")
print("When k = 3")
print("Classification Accuracy is ",classification_accuracy(euclidean_distance,3))
print("Macro Average Precision is ",macro_average_precision(euclidean_distance,3))
print("Macro Average Recall is ",macro_average_recall(euclidean_distance,3))
print("When k = 5")
print("Classification Accuracy is ",classification_accuracy(euclidean_distance,5))
print("Macro Average Precision is ",macro_average_precision(euclidean_distance,5))
print("Macro Average Recall is ",macro_average_recall(euclidean_distance,5))
print("When k = 7")
print("Classification Accuracy is ",classification_accuracy(euclidean_distance,7))
print("Macro Average Precision is ",macro_average_precision(euclidean_distance,7))
print("Macro Average Recall is ",macro_average_recall(euclidean_distance,7))

print("\nFor Manhattan Distance")
print("When k = 3")
print("Classification Accuracy is ",classification_accuracy(manhattan_distance,3))
print("Macro Average Precision is ",macro_average_precision(manhattan_distance,3))
print("Macro Average Recall is ",macro_average_recall(manhattan_distance,3))
print("When k = 5")
print("Classification Accuracy is ",classification_accuracy(manhattan_distance,5))
print("Macro Average Precision is ",macro_average_precision(manhattan_distance,5))
print("Macro Average Recall is ",macro_average_recall(manhattan_distance,5))
print("When k = 7")
print("Classification Accuracy is ",classification_accuracy(manhattan_distance,7))
print("Macro Average Precision is ",macro_average_precision(manhattan_distance,7))
print("Macro Average Recall is ",macro_average_recall(manhattan_distance,7))

"""### TASK 4:
Run your k-NN function for the values of k = 1, 2, 3, 4, 5, 6, 7 on test data. Do this for both the Euclidean distance and the Manhattan distance for each value of k. Plot three graphs displaying following:

*   k-values vs accuracy for both euclidean and manhattan distance (k-values on x-axis and accuracy values on y-axis)
*   k-values vs macro-average precision for both euclidean and manhattan distance (k-values on x-axis and precision values on y-axis)
* k-values vs macro-average recall for both euclidean and manhattan distance (k-values on x-axis and recall values on y-axis)

All of your graphs should be properly labelled.


"""

### YOUR CODE HERE ###
x = []
y1_list = []
y2_list = []
y3_list = []
y4_list = []
y5_list = []
y6_list = []
#Simple functions made in the previous task are called, looping through the k values of 1 to 7
for k in range (1,8): #loop from 1 to 7
  y1 = classification_accuracy(euclidean_distance,k)
  y2 = classification_accuracy(manhattan_distance,k)
  x.append(k)
  y1_list.append(y1)
  y2_list.append(y2)

for k in range (1,8): #loop from 1 to 7
  y1 = macro_average_precision(euclidean_distance,k)
  y2 = macro_average_precision(manhattan_distance,k)
  y3_list.append(y1)
  y4_list.append(y2)

for k in range (1,8): #loop from 1 to 7
  y1 = macro_average_recall(euclidean_distance,k)
  y2 = macro_average_recall(manhattan_distance,k)
  y5_list.append(y1)
  y6_list.append(y2)

figure, axis = plt.subplots(1,3)
axis[0].plot(x,y1_list,x,y2_list)
axis[0].legend(['Euclidean Distance','Manhattan Distance'],fontsize=12)
axis[0].set_title('K values vs Accuracy',fontsize=17)
axis[0].set_xlabel('K values',fontsize=14)
axis[0].set_ylabel('Accuracy',fontsize=14)

axis[1].plot(x,y3_list,x,y4_list)
axis[1].legend(['Euclidean Distance','Manhattan Distance'],fontsize=12)
axis[1].set_title('K values vs Macro-Average Precision',fontsize=17)
axis[1].set_xlabel('K values',fontsize=14)
axis[1].set_ylabel('Macro-Average Precision',fontsize=14)

axis[2].plot(x,y5_list,x,y6_list)
axis[2].legend(['Euclidean Distance','Manhattan Distance'],fontsize=12)
axis[2].set_title('K values vs Macro-Average Recall',fontsize=17)
axis[2].set_xlabel('K values',fontsize=14)
axis[2].set_ylabel('Macro-Average Recall',fontsize=14)
figure.set_size_inches(20, 6)

"""# Part 2: k-NN classifier using scikit-learn

### TASK 1:
In this part you have to use scikit-learn’s k-NN implementation to train and test your
classifier on the dataset used in Part 1. Run the k-NN classifier again for values of
k = 1, 2, 3, 4, 5, 6, 7 using both Euclidean and Manhattan distance. Use scikit-learn to calculate the accuracy, F1 score and confusion matrix for test data. Also present the
results as a graph with k values on x-axis and F1 score on y-axis for both distance metrics
in a single plot.
"""

### YOUR CODE HERE ###


listX = []
listY = []
f1score_euclidean = []
f1score_manhattan = []
x = [1,2,3,4,5,6,7]
for arr in training_data_dictionary["dress"]:
  listX.append(arr)
for i in range (len(listX)):
  listY.append("dress")
for arr in training_data_dictionary["shirts"]:
  listX.append(arr)
for i in range (len(training_data_dictionary["shirts"])):
  listY.append("shirts")
for arr in training_data_dictionary["shoes"]:
  listX.append(arr)
for i in range (len(training_data_dictionary["shoes"])):
  listY.append("shoes")
for arr in training_data_dictionary["pants"]:
  listX.append(arr)
for i in range (len(training_data_dictionary["pants"])):
  listY.append("pants")
#test data and training data in lists

def knnPredict(knnfunc):
  listPredict = []
  listActual = []
  z1 = (knnfunc.predict(test_data_dictionary["dress"]))
  for i in range (len(test_data_dictionary["dress"])):
    listActual.append("dress")
  z2 = knnfunc.predict(test_data_dictionary["shirts"])
  for i in range (len(test_data_dictionary["shirts"])):
    listActual.append("shirts")
  z3 = (knnfunc.predict(test_data_dictionary["shoes"]))
  for i in range (len(test_data_dictionary["shoes"])):
    listActual.append("shoes")
  z4 = (knnfunc.predict(test_data_dictionary["pants"]))
  for i in range (len(test_data_dictionary["pants"])):
    listActual.append("pants")
  listPredict = np.concatenate((z1,z2,z3,z4))  
  return (listPredict,listActual)
#function returns the actual classes and predicted classes 

print("For Euclidean Distance")
for i in range (1,8):
  knn = (KNeighborsClassifier (n_neighbors = i,p=2,algorithm = 'brute'))
  knn.fit(listX,listY)
  tuplePredic = knnPredict(knn)
  list_predict = tuplePredic[0]
  list_actual = tuplePredic[1]
  print('\nFor k = '+str(i)+' the accuracy score is')
  print(accuracy_score(list_actual,list_predict))
  print('The F1 score is :')
  print(f1_score(list_actual,list_predict,average='macro'))
  f1score_euclidean.append(f1_score(list_actual,list_predict,average='macro'))
  print('Confusion Matrix is: ')
  print(confusion_matrix(list_actual,list_predict))


print("\n\nFor Manhattan Distance")
for i in range (1,8):
  knn = (KNeighborsClassifier (n_neighbors = i,p=1))
  knn.fit(listX,listY)
  tuplePredic = knnPredict(knn)
  list_predict = tuplePredic[0]
  list_actual = tuplePredic[1]
  print('\nFor k = '+str(i)+' the accuracy score is')
  print(accuracy_score(list_actual,list_predict))
  print('The F1 score is :')
  print(f1_score(list_actual,list_predict,average='macro'))
  f1score_manhattan.append(f1_score(list_actual,list_predict,average='macro'))
  print('Confusion Matrix is: ')
  print(confusion_matrix(list_actual,list_predict))

print('\n')
plt.plot(x,f1score_euclidean,x,f1score_manhattan)
plt.legend(['Euclidean Distance','Manhattan Distance'],fontsize=12)
plt.title('K values vs F1 Score',fontsize=17)
plt.xlabel('K values',fontsize=14)
plt.ylabel('F1 Score',fontsize=14)

"""### TASK 2:
For this task you have been given a synthetic dataset of 1000 samples which is divided into 6 classes. Visualization of this dataset has also been given. Now you need to find the optimum value of k for this dataset. This can be done using GridSearchCV function provided by Scikit-learn. This function allows us to check easily for multiple values of k. You need to check for the values of k between 1 and 20 and report the best value of k.
"""

X, y = make_blobs(n_samples = 1000, n_features = 2, centers = 6, cluster_std = 2.5, random_state = 4)
plt.figure(figsize=(10,6))
plt.scatter(X[:,0], X[:,1], c=y, marker= 'o', s=50)
plt.show()

### YOUR CODE HERE ###
knn = (KNeighborsClassifier())

parameters = {
    'n_neighbors' :[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],
    'p' :[2]
}
print("Since the dataset is of 1000 samples, i decided to use 5 fold cross validation as 20 percent validation data on 80 percent training should suffice for this data set size\n")
print("For Euclidean distance, the optimal value of K using 5 fold cross validation is 12")
final = GridSearchCV(estimator = (KNeighborsClassifier()),param_grid=parameters,scoring='accuracy',cv=5)
final2 = final.fit(X,y)
print(final2.best_params_) 

parameters = {
    'n_neighbors' :[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],
    'p' :[1]
}

print('\nFor Manhattan Distance, the optimal value of K using 5 fold cross validation is 20')
final = GridSearchCV(estimator = (KNeighborsClassifier()),param_grid=parameters,scoring='accuracy',cv=5)
final2 = final.fit(X,y)
print(final2.best_params_)

"""#### ***Question: Is Feature Scaling required for the KNN Algorithm? Explain with proper justification.***

Ans: To answer this question, first we need to know what Feature Scaling means. Feature scaling is a technique used to normalize or standardize the data so that it it can be compared by machine learning algorithms to get the optimal result (In simple terms, you make sure that all the data is on the same scale). For example, if i make 100,000 in rupees but someone makes 90,000 in dollars, ofcourse the dollars person would be richer but we need to standardize it so that the machine learning algorithm is able to understand the difference.

Now, the way KNN algorithm works is that it computes the distance (either euclidean or manhattan e.t.c) between a test data point from all the training data and chooses k training instances which have the shortest distance from the test data point, and classifies the test data point according to the most repetetive class of the k samples. The important thing to note here is distance.

As an example, suppose you want to know how far you are from Lahore. Lets say you are 100 kilometers far from lahore. Now suppose you are 500 meters far from islamabad. If someone asks which is closer, the answer would be Islamabad, however, the KNN algorithm (if features scaling is not used) would choose lahore as 100 is less than 500 even though 100 (distance from lahore) kilometers is more than 500 meters (distance from Islamabad).

However, if feature scaling is used, 100000 meters from lahore and 500 meters from islamabad, now the Knn algorithm would choose Islamabad. 

This is why KNN requires feature scaling, so the algorithm can compare distances on a standard scale. 
"""