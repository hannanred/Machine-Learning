# -*- coding: utf-8 -*-
"""23110058_PA3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ncid8loo456nfo82OHA5LyJXiK3ozWvP

# Programming Assignment 3 : Linear Regression

## Instructions:

## Marks: 100
## Due Date: March, 13, 2022 

## Instructions 

*   Submit your code both as notebook file (.ipynb) and python script (.py) on LMS. The name of both files should be 'RollNo_PA3'. Failing to submit any one of them will result in the reduction of marks.

* The datasets required for this assignment have been uploaded to LMS. 

*   The code MUST be implemented independently. Any plagiarism or cheating of work from others or the internet will be immediately referred to the DC.

* 10% penalty per day for 3 days after due date. No submissions will be accepted

after  that.


* Use procedural programming style and comment your code properly.

* **Deadline to submit this assignment is 13/03/2022.**
* Make sure to run all blocks before submission.

### Goal: 

The goal of this assignment is to get you familiar with Linear Regression and to give hands on experience of basic python tools and libraries which will be used in implementing the algorithm.

### Note:

You are <font color="red">not allowed </font> to use scikit-learn or any other machine learning toolkit for part 1 and 2. You have to implement your own Linear Regression model from scratch. You may use Pandas, NumPy, Matplotlib and other standard python libraries
"""

import numpy as np
import pandas as pd
import matplotlib as plt
import matplotlib.pyplot as plt

"""# Part 1: Simple Linear Regression (10 Marks)

## Dataset: 
The Dataset for this part is provided in the included zip folder within the folder labelled "DataPart1". In case you are doing this assignment on colab, please upload the datafile to colab before starting.

## Pre-Processing:
The dataset you have been provided contains the marketing impact of a company via 3 advertisement mediums (Youtube, Facebook and Newspaper) on their sales. Data is the advertisement budget (in thousands of dollars) along with sales. Before you begin with Simple Linear Regression:
    
<ol>
<li> Plot Budget against Sales for each advertisement medium. </li>
<li> Identify which advertisement media has a linear relationship with Sales </li>
<li> Create a new Pandas data frame to extract this column along with corresponding sales data. </li>
<li> Split the data into train/test sets (80/20 Split).</li>
</ol>
"""

#Loading the data on lms
from google.colab import drive
drive.mount('/content/drive')

data_set= pd.read_csv('drive/MyDrive/DataPart1/marketing.csv')  # getting the data set by mounting the drive

# Plot 3 different scatterplots and do all the required pre-processing.
import matplotlib.pyplot as plt

figure, axis = plt.subplots(1,3)

axis[0].scatter(data_set.loc[:,"sales"],data_set.loc[:,"facebook"])
axis[0].set_title('Budget Facebook vs Sales (in $1000s)',fontsize=17) #Budget against sales, or y against x so 
axis[0].set_xlabel('Sales ',fontsize=14)
axis[0].set_ylabel('Budget',fontsize=14)

axis[1].scatter(data_set.loc[:,"sales"],data_set.loc[:,"youtube"])
axis[1].set_title('Budget Youtube vs Sales (in $1000s) ',fontsize=17)
axis[1].set_xlabel('Sales ',fontsize=14)
axis[1].set_ylabel('Budget ',fontsize=14)

axis[2].scatter(data_set.loc[:,"sales"],data_set.loc[:,"newspaper"])
axis[2].set_title('Budget newspaper vs Sales (in $1000s)',fontsize=17)
axis[2].set_xlabel('Sales ',fontsize=14)
axis[2].set_ylabel('Budget',fontsize=14)
figure.set_size_inches(20, 6)

print( "We can see that the news paper graph is randomly distributed and we can not see any correlation between budget and sales.\n For the facebook graph, we can see that there is some sort of a relationship between sales and budget, however, the variation in sales as budget increases is also increasing alot.\n For the youtube graph, we can see that sales seems to increase as budget increases and variation is little, so we will choose youtube")
new_set = pd.DataFrame({'Youtube':data_set.loc[:,"youtube"],'Sales':data_set.loc[:,"sales"]})
print("Test Data is") 
test_data = new_set.sample(frac=0.2) #sampling 20% randomly
print(test_data)
x_vals = []
y_vals = []
print("Training Data is")
training_data = new_set[~new_set.isin(test_data)].dropna() #getting the rest of 80% not in test_data
print(training_data)

"""## Tasks:

Implement Linear Regression from scratch to predict the sales of the company based on their advertisement budget for the media you selected. You will implement the following functions:

* Predict function:
    This function calculates the hypothesis for the input sample given the values of weights. 
    \begin{equation*}
        h(x,{\theta}) = \theta_0 + \theta_1 x,
        \end{equation*}

     where $${\theta} \in \mathbb{R}^{2} $$ is the weight vector given by $${\theta} = [ \theta_0, \theta_1]^T $$
"""

def predict(X,theta0,theta1):
    # X --> Data point
    predicted_y = theta0 + theta1*X   #predicted y = theta0 + theta1x
    return predicted_y

"""<li>Mean Square Error Function: This function calculates the cost of using weights as parameters for linear regression. The formula to calculate Mean Square Error is given below:</li>

\begin{equation*}
        J(\theta_0,\theta_1) =\frac{1}{2n} \sum_{i=1}^{n} (\hat{y}^i - y^i)^2,
        \end{equation*}
 where $y^i$ and $\hat{y}^i$ are the actual and predicted labels of the $i$-th training instance respectively and $n$ is the total number of training samples.
"""

def mean_square_error(X,Y,theta0,theta1):
    # X -> data point
    # Y -> True value corresponding to that point X
    n = Y.size #Total training points
    to_sum =0.0
    for iterator in range(n): #loop for all training points
      error = predict(X[iterator],theta0,theta1)-Y[iterator] #calculate the error (predicted y - actual y)
      to_sum = to_sum + (error**2) # add to the total sum the error squared 
    return ((to_sum))/(2*n) # divide by 2n

"""* Batch Gradient Descent: This function learns the values of weights when given as parameters the learning rate $\alpha$ and the number of iterations called epoch.
Experiment with different values to determine the best parameters.

For $j=0$ and $j=1$ repeat until convergence \{

$ \qquad \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j}  J(\theta_0,\theta_1)$

\}

"""

def gradient_descent(X,Y,alpha,epochs):
    # X -> train_x
    # Y -> train_y 
    
    theta0 = 0
    theta1 = 0
    J = list()
    n = Y.size
    for epoch in range(epochs): #number of times the gradient descent is run 

      to_sum = 0.0
      to_sum2 = 0.0
      for iterator in range(n): #for all data points
        to_sum = to_sum + (theta0+ predict(X[iterator],theta0,theta1) - Y[iterator]) #updating theta0, summing for all data points (theta0 + theta1Xi -yi)
        to_sum2 = to_sum2 + ((theta0+ predict(X[iterator],theta0,theta1) - Y[iterator])*X[iterator]) #updating theta1 ((theta0 + theta1x1 -yi)*xi)

      to_sum = (to_sum)/n #dividing by n
   
      to_sum2 = (to_sum2)/n
   
   
      theta0 = theta0 - (alpha*to_sum) #new theta = old theta - alpha*(calculated variable )
      theta1 = theta1 - (alpha*to_sum2)
      cost = mean_square_error(X,Y,theta0,theta1) #calculating the cost 
      J.append(cost)




    return J, theta0, theta1

alphas = np.linspace(0.000001,0.00001,num=100,endpoint=False) #generate 100 apha values from 0.000001 to 0.0000999999
epochs=[15001,17500,20000,25000,50000,75000,100000]#number of epochs from 15001 to 100000

X_train=np.array(training_data.loc[:,"Youtube"])
Y_train=np.array(training_data.loc[:,"Sales"])
list_answer = []

for alpha in alphas:   
    #looping to find the best alpha/epoch values
  for epoch in epochs:
    J, theta0,theta1 = gradient_descent(X_train,Y_train,alpha,epoch) 
    list_answer.append((J[-1],J,theta0,theta1,alpha,epoch))
 

answer = min(list_answer) #minimum sorts according to the first element of each tuple in the tuple list


J = answer[1]
print("Cost after convergence is: ",J[-1])
theta0 = answer[2]
theta1 = answer[3]
print("MSE On test data")
print(mean_square_error(np.array(test_data.loc[:,"Youtube"]),np.array(test_data.loc[:,"Sales"]),theta0,theta1))

"""* Use a value of $\alpha$ $<$ $0.00001$ and epochs $>$ 15000
* Your Minimum Cost on your test set should be around 11-15

### Question: 
Given the data, explain why such a large number of epochs and low learning rate is being used? Explain in terms of data and gradient descent function. Please answer by adding a markdown underneath this question.

ANSWER: Generally, we want to use a lower learning rate so small steps are taken along the curve and we are able to find the exact global minimum point for the gradient. However, since a lower learning rate is used, the steps taken are really small, that is why we need a really large number of epochs so we can take many or more small steps along the curve and find the exact global minimum. A smaller amount of epochs would mean that the function stops before the global minimum is reached.

### Plot Cost against Number of Epochs
"""

#Plotting
epochs = []
for x in range(100000):
  epochs.append(x)

fig,ax = plt.subplots()
ax.plot(epochs,J)
ax.set_xlabel('Number of Epochs')
ax.set_ylabel('Cost')
ax.set_title("Cost against Number of Epochs")
plt.title

"""### Plotting Linear Fit

- Using your learned paramters, plot a linear fit of Sales (Y-Axis) against Advertisement Budget (X-Axis).

Plot the original Scatterplot on the same graph as well.
"""

#Plotting
#original scatter plot was budget against sales, this plot is sales against budget 

x_vals = [0,20,50,70,100,150,200,230,260,290,300,350]
y_vals = []
for x in x_vals:
  y_vals.append(predict(x,theta0,theta1))

fig,ax = plt.subplots()
ax.scatter(data_set.loc[:,"youtube"],data_set.loc[:,"sales"])
ax.set_title('Sales against Advertisement Budget',fontsize=17)
ax.set_xlabel('Budget ',fontsize=14)
ax.set_ylabel('Sales ',fontsize=14)
ax.plot(x_vals,y_vals)

"""# Part 2: Multivariate Linear Regression (60 Marks)

Concrete is the most important material in civil engineering. Concrete compressive strength is an extremely important datapoint that engineers take into consideration while making decisions. To physically measure compressive strength is an expensive and costly process and is also dependent on age and ingredients. These ingredients include cement, blast furnace slag, fly ash, water, superplasticizer, coarse aggregate and fine aggregate. 

For this part, we will be prediciting the compressive strength of concrete using the mixture composition and age of a concrete mixture. You will find the data for this part in the folder labelled "DataPart2".

Data Attributes:

* Cement (Component 1) : Kg in a m3 Mixture
* Blast Furnace Slag (Component 2): Kg in a m3 Mixture
* Fly Ash (Component 3): Kg in a m3 Mixture
* Water (Compontnet 4): Kg in a m3 Mixture
* Superplasticizer (Component 5): Kg in a m3 Mixture
* Coarse Aggregate (Component 6): Kg in a m3 Mixture
* Fine Aggregate (Component 7): Kg in a m3 Mixture
* Age: (Days: 1~365)
* Concrete Compressive Strength (Output): MPa


Data Credits: Prof. I-Cheng Yeh

## Tasks:

* You are required to select the best features by drawing Scatter Plots/Heat Maps and using Pearson's correlation coefficent. (You may import a library for this)

* Please justify your selection (Removing any attribute or keeping all attributes) in a markdown box below this one (Add one)
"""

from google.colab import drive
drive.mount('/content/drive')
!pip install xlrd==1.2.0  #read excel rqeuired me to get 1.2.0 version of xlrd 

data_set= pd.read_excel('drive/MyDrive/DataPart2/Concrete.xls')

## Scatter Plot/Heat Map and Correlation Matrix
import seaborn as sns
import matplotlib.pyplot as plt
plt.figure(figsize = (10,10))
sns.heatmap(data_set.corr(),annot=True)
plt.show()   #scatterplot heatmap and correlation matrix, this was inspired from a post on the seaborn website

"""#ANSWER
Using 90% as a threshold value for correlation, We can see that no two inputs are correlated to each other by more than 90%, (or 0.90/0.90), so, we will not be dropping any inputs from the data set (Atleast in the case where two inputs are so highly correlated that they can be interpreted as being exactly the same).
We can see that fly ash and blast furnace slag does have low correlation with the label, but we will not be dropping them (we set our threshold at 90%, if it was 85% we would)as It may have an effect on the predicted label. It is to be noted that it may possibly act as noise, but we believe that the noise would be sufficiently small to not affect the predicted label by much.

* Data Normalization: Normalize the Dataset by subtracting the mean of each feature from the feature value and then divide by the standard deviation of that feature:

\begin{equation*}
        x_{\rm norm} = \frac{x - {\text{mean}}(x)}{{\rm std}(x)}
    \end{equation*}
(For normalization of test set, use mean and standard deviation of training set.)
"""

#Data Normalization


test_data = data_set.sample(frac=0.2)
training_data = data_set[~data_set.isin(test_data)].dropna() #dividing into test data and training data 


test_data_set = np.array(test_data)
training_data_set = np.array(training_data)

test_data_og = test_data_set
training_data_og = training_data_set
for x in range(8): #simply for all columns, finding their mean and sd on the training set, and normalizing every variable by subtracting the mean and sdev for both training and test set
  list_mean= (np.mean(training_data_set[:,x]))
  list_stdev = (np.std(training_data_set[:,x])) 
  test_data_set[:,x] = (test_data_set[:,x] - list_mean)/list_stdev
  training_data_set[:,x] = (training_data_set[:,x] - list_mean)/list_stdev



test_data = pd.DataFrame(test_data_set)
training_data = pd.DataFrame(training_data_set)

"""* Implement Predict Function, Mean Square Error and Batch Gradient Descent Function as explained in Part 1 for multivariate linear regression.


"""

## Implementation

#For the purpose of multivariate regression, i added theta0 to the theta1 parameter and added a dummy xi variable with unit 1 
#for example previously if theta0 was 2 and theta1 was [1,2,3] and x was [2,3,4] now x is [1,2,3,4] and theta 1 is [2,1,2,3]
def predict(X,theta1):
    return np.dot(theta1,X)  #return matrix multiplication

def mean_square_error(X,Y,theta1):
  return (np.sum(np.square((predict(X,theta1)) - Y)))/(2*(Y.size)) #same as in univariate case, returns the mean square error 

def gradient_descent(X, Y, alpha,epochs):
    n = Y.size  # number of training examples
    J = list()  # list to store cost
    X = X.transpose() 
    theta1 = [0,0,0,0,0,0,0,0,0]
    for epoch in range(epochs):


      for iterator in range(9):
        pd = predict(X,theta1)
        summation = np.sum((pd-Y) * X[iterator])            
        theta1[iterator] = theta1[iterator] - ((alpha/n) * summation)    
   
      cost = mean_square_error(X,Y,theta1)

      J.append(cost)

    return J,theta1

"""* Plot the No. of Epochs (y-axis) vs Training Loss (x-axis)"""

#Plotting
alpha=0.001
epochs=5000

X = np.array((1 + training_data.loc[:,training_data.columns!=8]))
ny = np.array(1)

finale = []
for iterator in range(824):
  finale.append(np.append(ny,X[iterator]))

X = np.array(finale)

Y = np.array(training_data.loc[:,8])



J,theta1 = gradient_descent(X,Y,alpha,epochs)

print("Cost after convergence is: ",J[-1])
print(J)

y_vals = np.arange(5000)

fig,ax = plt.subplots()
ax.set_title('Number of Epochs vs Training Loss',fontsize=17)
ax.set_xlabel('Training Loss ',fontsize=14)
ax.set_ylabel('Number of Epoch ',fontsize=14)
ax.plot(J,y_vals)

"""* Measure Mean Square Error of your test set using your learned rate. 

"""

## Measure
X = np.array((1 + test_data.loc[:,test_data.columns!=8]))
ny = np.array(1)
finale = []
for iterator in range(206):
  finale.append(np.append(ny,X[iterator]))

X = np.array(finale)
X = np.transpose(X)
Y = np.array(test_data.loc[:,8])
mean_square_error(X,Y,theta1)

"""#### Question 1: Mention the best values of Alpha and Numb Of Epochs:

Answer 1:
<ol> 

<li> 
Alpha: 0.001

</li>
<li> 
Numb of Epochs: 
5000
</li>
</ol>

#### Question 2: What is the Mean_Square_Error of your model? Suggest Possible ways to improve the accuracy with a small description of each avenue. 

Answer 2:
The Mean_Square_Error of my model on the test data is 51.18
. One possible way and the most obvious one is to run the model on finer values of alpha (like 0.0001 or or 0.000001). They will however require larger amount of epochs and will take more time as it is computationally exhaustive.
Another thing to try out may be to carry out feature selection again, this time with different threshold values like 85% or 75%, it may turn out that some features were acting as noise. 
We may also try adding more data either by recording it ourselves, or by changing the train-test split to 90-10. However, this may cause problems down the line as the model may not necessarily be generalizable. Similarly, using cross-validation techniques we may get a better MSE on the test data but again, the model itself might not be generalizable. Many more methods are also possible through feature engineering such as feature transformation which may approve accuracy. It is to be noted that we should avoid overfitting while choosing one of the techniques mentioned

# Part 3: Regularized Linear Regression (30 Marks)
"""

from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn.metrics import mean_squared_error

"""Regularization is a technique that assumes smaller weights generate simple models and helps avoid overfitting. In this part, you will be using various regularization techniques on the Cement Dataset (Provided in Part 2).

## Tasks:

Implement the least squares [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html), [Lasso Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso),[Ridge Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge), and [Elastic Net Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet) using [scikit-learn](https://scikit-learn.org/stable/index.html). You are required to:

* Try out different values of regularization paramters (alpha in scikit-learn document) and use the validation set to determine the best value of regularization parameter by computing validation loss using [Mean Squared Error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html).

* For Ridge Regression and Elastic Net Regression, plot regularization coefficients on the x-axis and learned parameters $\theta$ on the y-axis. Please read this [blog](https://scienceloft.com/technical/understanding-lasso-and-ridge-regression/) as reference.

* After evaluating the best value of the regularization parameter, use the [Mean Squared Error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) to compute the loss on the test set for each regression.
"""

training_data_set = training_data_og
test_data_set = test_data_og
test_data_set = pd.DataFrame(test_data_set)
training_data_set = pd.DataFrame(training_data_set)
X = np.array(training_data_set.loc[:,training_data.columns!=8])
Y = np.array(training_data_set.loc[:,8])
test_X = np.array((test_data_set.loc[:,test_data.columns!=8]))
test_Y = np.array(test_data_set.loc[:,8])



reg = LinearRegression().fit(X, Y)
predicted = reg.predict(test_X)
MSE = mean_squared_error(test_Y, predicted, sample_weight = None, squared=True)/2
print("Mean Squared Error for simple Linear regression is:",MSE)


alphas = np.logspace(-10,10,10000)
mse_and_alpha = []
learned_param= []
alpha = []

for x in alphas:
    clf = Ridge(alpha=x)
    clf.fit(X, Y)
    clf_test = clf.predict(test_X)
    learned_param.append(clf.coef_)
    predicted = clf.predict(test_X)
    MSE = mean_squared_error(test_Y, predicted, sample_weight = None, squared=True)/2
    alpha.append(x)
    mse_and_alpha.append((MSE,x))
    
print("Best value for Ridge regularization parameter is",(min(mse_and_alpha))[1]," with corresponding Mean square error :",(min(mse_and_alpha))[0])


figure, axis = plt.subplots(1,2)
axis[0].plot(alpha, learned_param)
axis[0].set_xscale("log")
axis[0].set_xlabel('Alpha',fontsize=14)
axis[0].set_ylabel('Theta (Learned Parameters)',fontsize=14)
axis[0].set_title("Learned Parameters vs Regularization Coefficients Ridge Regression",fontsize=17)




alphas = np.logspace(-10,10,10000)
mse_and_alpha=[]

for x in alphas:
    clf = Lasso(alpha=x)
    clf.fit(X, Y)
    clf_test = clf.predict(test_X)
    res_test = clf.predict(test_X)
    MSE = mean_squared_error(test_Y, res_test, sample_weight = None, squared=True)/2
    mse_and_alpha.append((MSE,x))
    
print("Best value for Lasso regularization parameter is",(min(mse_and_alpha))[1]," with corresponding Mean square error :",(min(mse_and_alpha))[0])



alphas = np.logspace(-10,10
                     ,10000)
l1_ratios = np.linspace(0.01,1,100,endpoint=True)
mse_and_alpha = []
alpha = []
learned_param = []
from sklearn.linear_model import ElasticNet
for l1 in l1_ratios:
  for x in alphas:
      reg = ElasticNet(random_state=0,alpha=x,l1_ratio = l1)
      reg.fit(X,Y)
      predicted = reg.predict(test_X)
      learned_param.append(reg.coef_)
      MSE = mean_squared_error(test_Y, predicted, sample_weight = None, squared=True)/2
      mse_and_alpha.append((MSE,x,l1))
      alpha.append(x)
print("Best value for Elastic Net Regression alpha parameter is",(min(mse_and_alpha))[1]," with corresponding Mean square error :",(min(mse_and_alpha))[0]," and l1_ratio: ", (min(mse_and_alpha))[2])

l1 = (min(mse_and_alpha))[2]
alpha = []
learned_param = []

for x in alphas:
  reg = ElasticNet(random_state=0,alpha=x,l1_ratio = l1)
  reg.fit(X,Y)
  predicted = reg.predict(test_X)
  learned_param.append(reg.coef_)
  MSE = mean_squared_error(test_Y, predicted, sample_weight = None, squared=True)/2
  alpha.append(x)


axis[1].plot(alpha, learned_param)
axis[1].set_xscale("log")
axis[1].set_xlabel('Alpha',fontsize=14)
axis[1].set_ylabel('Theta (Learned Parameters)',fontsize=14)
axis[1].set_title("Learned Parameters vs Regularization Coefficients Elastic Net Regression",fontsize=17)
figure.set_size_inches(20, 6)

"""### Question: What is the difference between Ridge Regression and Lasso Regression

Ans: Ridge regression adds penalty terms associated with square of the norm of the coeffiecients, while lasso regression uses the absoltue sum of the coefficients for the pentalty term (it uses L^1 instead of L^2 in ridge). Due to this difference, in lasso as lambda increases coefficients are penalized more and sometimes even to 0 (for irrelevant features), while in Ridge irrelevant features do get small but are not zero. 
"""