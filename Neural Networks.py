# -*- coding: utf-8 -*-
"""23110058_partA_PA5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CjPJLSWrRFPwhbliTtoChWh9gM1IIEMI

## CS535/EE514 - Spring 2022 - Assignment 5 



#### Important Instructions and Submission Guidelines:
- ## Important Instructions and Submission Guidelines:
- Submit your code both as notebook file (.ipynb) and python script (.py) on LMS. Naming convention for submission of this notebook is `RollNumber_partA_PA5.ipynb` where. For example: `23100042_partA_PA5.ipynb`
- All the cells <b>must</b> be run once before submission. If your submission's cells are not showing the results (plots etc.), marks wil be deducted
- Only the code written within this notebook's marked areas will be considered while grading. No other files will be entertained
- You are advised to follow good programming practies including approriate variable naming and making use of logical comments.


The university honor code should be maintained. Any violation, if found, will result in disciplinary action.

Often in pratical situations, raw machine learning architecture and code is hidden behind libraries and simplfied toolkits. The purpose of this assignment is to lift that curtain and get you hands-on exprience working with the mathematical fundamentals of neural network architectures. After this, you'll know exactly how a network leverages 'gradient descent' to find optimal solutions and how forward and backward passes are implemented mathematically and in code.
"""

rollnumber = 23110058

"""This is the dataset that we are goint to use. As you can see there are two classes and there is a non-linear relationship between them. We are going to build a Neural Network from scratch to learn this non-linear relationship."""

from sklearn.datasets import make_moons
import matplotlib.pyplot as plt
import numpy as np

points = 500

X_train, y_train = make_moons(points, noise=0.10)

y_train = y_train.reshape(points,1)
print(y_train.shape,X_train.shape)

plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap=plt.cm.Spectral);
plt.gca().set(xlabel='Feature 1', ylabel='Feature 2')

print(len(y_train))

"""In the first part, you are going to build a neural network with 1 input layer and 1 output layer.

In the second part, you will be builidng neural network with 1 input layer, 1 hidden layer and 1 output layer.

You will be implementing forward pass and most importantly backward pass. Coding these algorithms will help solidfy these concepts which are at the core of modern day Machine Learning and Deep Learning.

## Part1

Use numpy random [normal](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html) to intialize weight matrix of the right shape.

Neural Network that you are going to implement will have one input layer of 2 neurons and an output layer of one neuron
"""

from IPython.display import Image
Image('graph1.png')

def initialize_weight():
    '''Initializes all weights based on standard normal distribution'''
    biases = np.zeros(shape=(1,))
    size = 2
    weight=[]
    weight.append(np.random.normal(size=(2,1)))
    weight.append(biases)
    return weight

"""Activation Function"""

def sigmoid(x):
    # implement sigmoid function
    return 1/(1+(np.exp(-x)))

"""Metrics for evaluating model"""

import math
def cross_entropy_loss(y_pred, y_true):
  t1=y_true*np.log(y_pred+0.0000000000000000000000000000000001)
  t2 = np.sum(t1)
  t3=-t2
  ans = t3
  return ans

def accuracy1(y_pred, y_true):
  true_count = 0
  for x in range(len(y_true)):
    if y_true[x]==y_pred[x]:
      true_count = true_count+1
  return (true_count/len(y_true))

"""Forward Pass. To make your code computationally fast try to use dot product (vectorization) instead of for loops. Also don't forget to use activation function."""

def forward_pass(weight, Xs):
    # implement forward pass function 
    '''Executes the feed forward algorithm. "Xs" is the input to the network.
        Returns "layer_activations", which is a list of all layer outputs'''
    
    activations_out = np.matmul(Xs,weight[0]) + weight[1]
    
    activations_out = sigmoid(activations_out)
  
    layer_activations=activations_out
  
    return layer_activations

"""Backward Pass. This is the most important step in learning process where you update your weights so that thel model fits your data."""

def backward_pass(layer_activations, Ys,weight,Xs):
    # implement backward pass function 

    '''Executes the backpropogation algorithm. "Ys" is the ground truth/labels, 
        "layer_activations" are the return value of the forward pass step.
    Returns "deltas", which is a list containing weight update values for all layers'''   
    
    deltas = []
    deltas.append(np.matmul(Xs.T,layer_activations- Ys))
    deltas.append((layer_activations-Ys))
    return deltas

def weight_update(weight, deltas, lr):
    '''After executing the gradient descent algorithm "deltas" is return value of the backward pass step
        "lr" is the learning rate
        Update the weights of the model and return updated weights
    '''
    weight[0]=weight[0]-lr*deltas[0]
    
    temp = lr * np.sum(deltas[1], axis=0)
    weight[1]=weight[1] - temp
    

    return weight

"""Predict function is similar to forward propagation except you need to now return numpy array containing binary class labels 0 and 1 because there are two classes in the dataset. Set a threshold for this classification."""

def predict(Xs, weight):
    '''Returns the model predictions (output of the last layer) for the given "Xs".'''
    
    activations_out = np.matmul(Xs,weight[0]) + weight[1]
    
    activations_out = sigmoid(activations_out)
  
    layer_activations=activations_out

    temp =layer_activations
    final = []
    for x in temp:
      if x < 0.5:
        final.append(0)
      if x>=0.5:
        final.append(1)
    predictions=final
    return np.array(predictions)

"""Implement the fit funciton. You need to use all above functions to fit and train the model in following steps.

*   Weight initialization
*   Forward Pass
*   Backward Pass
*   Weight Update
*   Compute Cost, Accuracy and append it to the list


You should use the lecture [slides](https://www.zubairkhalid.org/ee514/2022/notes12.pdf) as reference for the equations. 

"""

def fit(Xs, Ys , epochs , lr = 1e-3 ):
    losses = []
    accuracy = []
    weights = initialize_weight()
    for epoch in range(epochs):
     
      layer_act = forward_pass(weights, Xs)
      # print(layer_act)
      deltas = backward_pass(layer_act, Ys,weights,Xs)
      
      weights = weight_update(weights, deltas, lr)
      # print(weights)
      predictions = predict(Xs,weights)
      Ys=np.array(Ys)
      predictions=np.array(predictions)

      los = cross_entropy_loss(layer_act,Ys)
      losses.append(los)
  
        # print(cross_entropy_loss(predictions,Ys))
        # print(losses)
        # print(predictions)
        # print(losses)
      acc = accuracy1(predictions,Ys)
      accuracy.append(acc)

      if epoch%100 == 0:
          print("epoch ",epoch,"loss ",los)
        
      
    return losses,accuracy,weights
losses, accuracy, weights = fit(X_train,y_train,1000)
print(accuracy)

"""Expected Loss after training is below 90

Plot the following graphs:

– Training loss (y-axis) vs no. of epochs (x-axis).

– Training accuracy (y-axis) vs no. of epochs (x-axis).


Predict the labels of test data using the predict function. Skeleton code for this function is already given to you. You may find np.argmax or np.where useful to extract labels.
"""

import matplotlib.pyplot as plt
x_axis = np.arange(1000)

plt.plot(x_axis,losses)
plt.xlabel("Epoch Number")
plt.ylabel("Cross Entropy Loss")

plt.plot(x_axis,accuracy)
plt.xlabel("Epoch Number")
plt.ylabel("Accuracy")

"""Once you are done training. Run the following cell to plot the decision boundary."""

fig, ax = plt.subplots()
x_min, x_max = X_train[:, 0].min() - .5, X_train[:, 0].max() + .5
y_min, y_max = X_train[:, 1].min() - .5, X_train[:, 1].max() + .5

def plot_decision_boundary(pred_func,weight, x_min, x_max, y_min, y_max, cmap, ax):
    h = 0.01
    # Generate a grid of points with distance h between them
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    # Predict the function value for the whole gid
    Z = pred_func(np.c_[xx.flatten(), yy.flatten()],weight)
    Z = Z.reshape(xx.shape)
    # Plot the contour
    ax.contourf(xx, yy, Z, cmap=cmap, alpha=0.5)

plot_decision_boundary(predict, weights,
                       x_min, x_max, y_min, y_max, 
                       plt.cm.Spectral, ax)
ax.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap=plt.cm.Spectral);
ax.set(xlabel='Feature 1', ylabel='Feature 2', title='Neural Network Classifier {}'.format(rollnumber));

"""A limitation we clearly see of this decision boundry is that it is linear, however, we know that a non linear relationship exists from the visualization of the data

## Part2

In part 2 you will add a hidden layer of 10 neurons to your neural network architecure and re-implement all of the functions above. You can copy cells. Comment on the difference between the decision boundary in both parts that you observed.

This is the Neural Network architecture for this part
"""

from IPython.display import Image
Image('graph.png')

"""Expected loss after training for this part is less than 20"""

def initialize_weight2():
    '''Initializes all weights based on standard normal distribution'''
    inp = 2
    hidden = 10
    biases_hidden = np.zeros(shape=(10,))
    biases_output = np.zeros(shape=(1,))
    weight1 = np.random.normal(size=(10,1))
    weight2 = np.random.normal(size=(2,10))
    weight=[]
    weight.append(weight2)
    weight.append(weight1)
    weight.append(biases_hidden)
    weight.append(biases_output)
    return weight
def forward_pass(weight, Xs): #Hidden layer at index 0, output layer at index 1
    # implement forward pass function 
    '''Executes the feed forward algorithm. "Xs" is the input to the network.
        Returns "layer_activations", which is a list of all layer outputs'''
  
    activations_hidden = np.matmul(Xs,weight[0]) + weight[2]
    activations_hidden = sigmoid(activations_hidden)
    
    activations_out = np.matmul(activations_hidden,weight[1]) + weight[3]
    
    activations_out = sigmoid(activations_out)
  
    layer_activations=[activations_hidden,activations_out]
  
    return layer_activations
def backward_pass(layer_activations, Ys,weight,Xs):
    # implement backward pass function 

    '''Executes the backpropogation algorithm. "Ys" is the ground truth/labels, 
        "layer_activations" are the return value of the forward pass step.
    Returns "deltas", which is a list containing weight update values for all layers'''   
    deltas = []
    temp = layer_activations[1]-Ys
    x1= temp
    #x1 is output of output layer
    tempz = np.multiply(layer_activations[0],(1-layer_activations[0]))

    tempx = np.multiply(x1,weight[1].T)
    zed4 = np.multiply(tempz,tempx)
    
    deltas.append(x1)
    deltas.append(zed4)


      
    return deltas
def weight_update(weight, deltas, lr,inputs):
    '''After executing the gradient descent algorithm "deltas" is return value of the backward pass step
        "lr" is the learning rate
        Update the weights of the model and return updated weights
    '''
    
    weight[0]=weight[0]-lr*np.matmul(inputs[0].T,deltas[1])

    
    temp = lr * np.sum(deltas[1], axis=0)
    weight[2]=weight[2] - temp
    
    
    weight[1]=weight[1]-lr*np.matmul(inputs[1].T,deltas[0])

    temp = lr * np.sum(deltas[0],axis=0)
    weight[3]=weight[3]-temp
   


    return weight
def predict(Xs, weight):
    '''Returns the model predictions (output of the last layer) for the given "Xs".'''
     
    res = forward_pass(weight,Xs)
    final = []
    for x in res[1]:
      if x < 0.5:
        final.append(0)
      if x>=0.5:
        final.append(1)
    predictions=final
    return np.array(predictions)

def fit(Xs, Ys , epochs , lr = 1e-3 ):
   
    
    losses = []
    accuracy = []
    weights = initialize_weight2()
    
    for epoch in range(epochs):
    
      layer_act = forward_pass(weights, Xs)
     
      deltas = backward_pass(layer_act, Ys,weights,Xs)
      
      inputs = [Xs] + layer_act[:-1]
      weights = weight_update(weights, deltas, lr,inputs)
     
     
      predictions = predict(Xs,weights)
      Ys=np.array(Ys)
      predictions=np.array(predictions)

      los = cross_entropy_loss(layer_act[1],Ys)
      losses.append(los)
  
       
      acc = accuracy1(predictions,Ys)
      accuracy.append(acc)
      
      if epoch%100 == 0:
          print("epoch ",epoch,"loss ",los)
        
      
    return losses,accuracy,weights
losses, accuracy, weights = fit(X_train,y_train,10000)
print(accuracy)

import matplotlib.pyplot as plt
x_axis = np.arange(10000)

plt.plot(x_axis,losses)
plt.xlabel("Epoch Number")
plt.ylabel("Cross Entropy Loss")

plt.plot(x_axis,accuracy)
plt.xlabel("Epoch Number")
plt.ylabel("Accuracy")

fig, ax = plt.subplots()
x_min, x_max = X_train[:, 0].min() - .5, X_train[:, 0].max() + .5
y_min, y_max = X_train[:, 1].min() - .5, X_train[:, 1].max() + .5

def plot_decision_boundary(pred_func,weight, x_min, x_max, y_min, y_max, cmap, ax):
    h = 0.01
    # Generate a grid of points with distance h between them
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    # Predict the function value for the whole gid
    Z = pred_func(np.c_[xx.flatten(), yy.flatten()],weight)
    Z = Z.reshape(xx.shape)
    # Plot the contour
    ax.contourf(xx, yy, Z, cmap=cmap, alpha=0.5)

plot_decision_boundary(predict, weights,
                       x_min, x_max, y_min, y_max, 
                       plt.cm.Spectral, ax)
ax.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap=plt.cm.Spectral);
ax.set(xlabel='Feature 1', ylabel='Feature 2', title='Neural Network Classifier {}'.format(rollnumber));

"""With an added hidden layer, this time around the decision boundry is non linear, it allows for a better fit and the classes are better seperated
This is due to adding a hidden layer in the network.
"""